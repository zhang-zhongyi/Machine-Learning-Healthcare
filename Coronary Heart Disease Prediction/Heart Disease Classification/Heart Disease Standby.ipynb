{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g\n",
    "#in ipython file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "748/2:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "748/3:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/1:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "749/2:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/3: df0.isnull().sum()\n",
    "749/4: df0.head()\n",
    "749/5: df0['cigsPerDay']=df0['cigsPerDay'].fillna(mean[df0['cigsPerDay']])\n",
    "749/6: df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "749/7: df0.isnull().sum()\n",
    "749/8: tester.isnull().sum()\n",
    "749/9: tester=pd.read_csv('test.csv')\n",
    "749/10: tester.isnull().sum()\n",
    "749/11: terster\n",
    "749/12: tester\n",
    "749/13:\n",
    "tester['feature']=tester['feature'].fillna(tester['feature'].mean())\n",
    "tester['feature']\n",
    "749/14:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "749/15: df0.isnull().sum()\n",
    "749/16:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "749/17: df0.isnull().sum()\n",
    "749/18:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/19: df0.isnull().sum()\n",
    "749/20:\n",
    "clu = df0.drop(['education','glucose','TenYearCHD'],axis=1,inplce=True)\n",
    "clu.head()\n",
    "749/21:\n",
    "clu = df0.drop(['education','glucose','TenYearCHD'],axis=1,inplace=True)\n",
    "clu.head()\n",
    "749/22:\n",
    "clu = df0.drop(['education','glucose','TenYearCHD'],axis=1,inplace=True)\n",
    "clu\n",
    "749/23:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "749/24:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/25: df0.head()\n",
    "749/26: df0.isnull().sum()\n",
    "749/27:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/28: df0.isnull().sum()\n",
    "749/29: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1,inplace=True)\n",
    "749/30: head\n",
    "749/31: clu\n",
    "749/32:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "749/33:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/34: df0.head()\n",
    "749/35: df0.isnull().sum()\n",
    "749/36:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/37: df0.isnull().sum()\n",
    "749/38: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "749/39: clu\n",
    "749/40: clu.dtype\n",
    "749/41: clu.dtype()\n",
    "749/42: dtype(clu)\n",
    "749/43: clu.dtypes()\n",
    "749/44: clu.dtypes\n",
    "749/45: df0.dtypes()\n",
    "749/46: df0.dtypes\n",
    "749/47:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "749/48:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/49: df0.head()\n",
    "749/50: df0.isnull().sum()\n",
    "749/51:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/52: df0.isnull().sum()\n",
    "749/53: df0.dtypes\n",
    "749/54:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "749/55: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "749/56: clu.dtypes\n",
    "749/57: clu.isnull().sum()\n",
    "749/58:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "749/59:\n",
    "\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',\n",
    "                    max_iter = 400, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(clu)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for 2015 before PCA')\n",
    "plt.xlabel('Association')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()\n",
    "749/60:\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',\n",
    "                    max_iter = 400, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(clu)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for 2015 before PCA')\n",
    "plt.xlabel('Association')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()\n",
    "749/61:\n",
    "#k = 4\n",
    "clukmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                   n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clukmeans)\n",
    "\n",
    "plt.scatter(clu[y_clu == 0, 0], clu[y_clu == 0, 1], s = 35, \n",
    "            c = 'orange')\n",
    "plt.scatter(clu[y_clu == 1, 0], clu[y_clu == 1, 1], s = 35, \n",
    "            c = 'blue')\n",
    "plt.scatter(clu[y_clu == 2, 0], clu[y_clu == 2, 1], s = 35, \n",
    "            c = 'green')\n",
    "plt.scatter(clu[y_clu == 3, 0], clu[y_clu == 3, 1], s = 35, \n",
    "            c = 'purple')\n",
    "\n",
    "#Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 100, \n",
    "            c = 'red', label = 'Centroide')\n",
    "\n",
    "# xk4=clu[y_clu == 1, 0]\n",
    "# yk4=clu[y_clu == 1, 1]\n",
    "# mydict = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict)\n",
    "# l4 = [names2015[i] for i in (40,  53, 180, 340)]\n",
    "# for x, y, company in zip(xk4, yk4, l4):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease dataset',fontsize=17)\n",
    "plt.legend()\n",
    "749/62:\n",
    "#k = 4\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clu)\n",
    "\n",
    "plt.scatter(clu[y_clu == 0, 0], clu[y_clu == 0, 1], s = 35, \n",
    "            c = 'orange')\n",
    "plt.scatter(clu[y_clu == 1, 0], clu[y_clu == 1, 1], s = 35, \n",
    "            c = 'blue')\n",
    "plt.scatter(clu[y_clu == 2, 0], clu[y_clu == 2, 1], s = 35, \n",
    "            c = 'green')\n",
    "plt.scatter(clu[y_clu == 3, 0], clu[y_clu == 3, 1], s = 35, \n",
    "            c = 'purple')\n",
    "\n",
    "#Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 100, \n",
    "            c = 'red', label = 'Centroide')\n",
    "\n",
    "# xk4=clu[y_clu == 1, 0]\n",
    "# yk4=clu[y_clu == 1, 1]\n",
    "# mydict = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict)\n",
    "# l4 = [names2015[i] for i in (40,  53, 180, 340)]\n",
    "# for x, y, company in zip(xk4, yk4, l4):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease dataset',fontsize=17)\n",
    "plt.legend()\n",
    "749/63: clu=clu.values\n",
    "749/64:\n",
    "#k = 4\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clu)\n",
    "\n",
    "plt.scatter(clu[y_clu == 0, 0], clu[y_clu == 0, 1], s = 35, \n",
    "            c = 'orange')\n",
    "plt.scatter(clu[y_clu == 1, 0], clu[y_clu == 1, 1], s = 35, \n",
    "            c = 'blue')\n",
    "plt.scatter(clu[y_clu == 2, 0], clu[y_clu == 2, 1], s = 35, \n",
    "            c = 'green')\n",
    "plt.scatter(clu[y_clu == 3, 0], clu[y_clu == 3, 1], s = 35, \n",
    "            c = 'purple')\n",
    "\n",
    "#Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 100, \n",
    "            c = 'red', label = 'Centroide')\n",
    "\n",
    "# xk4=clu[y_clu == 1, 0]\n",
    "# yk4=clu[y_clu == 1, 1]\n",
    "# mydict = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict)\n",
    "# l4 = [names2015[i] for i in (40,  53, 180, 340)]\n",
    "# for x, y, company in zip(xk4, yk4, l4):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease dataset',fontsize=17)\n",
    "plt.legend()\n",
    "749/65: clu\n",
    "750/1:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "750/2:\n",
    "fundamentals = pd.read_csv('fundamentals.csv',index_col = 0)\n",
    "price_split=pd.read_csv('prices-split-adjusted.csv')\n",
    "securities = pd.read_csv('securities.csv')\n",
    "sectors = securities['GICS Sector'].unique()\n",
    "sub_industry = securities['GICS Sub Industry'].unique()\n",
    "accounts = fundamentals.columns.values[2:]\n",
    "750/3: fundamentals = fundamentals.rename(columns={'Period Ending': 'date'})\n",
    "750/4: fundamentals = fundamentals.rename(columns={'Ticker Symbol': 'symbol'})\n",
    "750/5: new_data=pd.merge(fundamentals, price_split, on='date', how='left')\n",
    "750/6: new_data1=new_data[new_data[\"symbol_x\"]==new_data[\"symbol_y\"]]\n",
    "750/7: new_data1.drop(['symbol_y'], axis=1,inplace=True)\n",
    "750/8: new_data1=new_data1.rename(columns={'symbol_x':'symbol'})\n",
    "750/9: securities=securities.rename(columns={'Ticker symbol':'symbol'})\n",
    "750/10: new_data2=pd.merge(new_data1, securities, on='symbol', how='left')\n",
    "750/11:\n",
    "#create the For year column again to get rid of null values\n",
    "new_data2['For Year']=new_data2.date.str.split(\"-\",expand=True,)[0]\n",
    "750/12:\n",
    "#drop Equity Earnings/Loss Unconsolidated Subsidiary because 60% of the data is zero??\n",
    "#new_data2[new_data2['Equity Earnings/Loss Unconsolidated Subsidiary']==0]\n",
    "new_data2.drop(['CIK','Security','SEC filings','Date first added','Equity Earnings/Loss Unconsolidated Subsidiary','Address of Headquarters'], axis=1,inplace=True)\n",
    "750/13: new_data2['PE'] = new_data2['close'] / new_data2['Earnings Per Share']\n",
    "750/14:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "new_data2['date']=pd.to_datetime(new_data2['date'])\n",
    "750/15:\n",
    "#missing EPS and shares outstanding valuses as zero because its possible the company wasnt public\n",
    "new_data2[['Earnings Per Share']] = new_data2[['Earnings Per Share']].fillna(value=0)\n",
    "new_data2[['Estimated Shares Outstanding']] = new_data2[['Estimated Shares Outstanding']].fillna(value=0)\n",
    "new_data2[['PE']] = new_data2[['PE']].fillna(value=0)\n",
    "750/16:\n",
    "#negative P/E ratio indicates low earnings/profit\n",
    "new_data2['trend']=np.where(new_data2['PE']>0, '1', '0')\n",
    "750/17: new_data2.isnull().sum()\n",
    "750/18: new_data2['trend']=np.where(new_data2['PE']>0, '1', '0').astype(int)\n",
    "750/19:\n",
    "print(new_data2.columns.get_loc('symbol'))\n",
    "print(new_data2.columns.get_loc('For Year'))\n",
    "print(new_data2.columns.get_loc('date'))\n",
    "print(new_data2.columns.get_loc('GICS Sector'))\n",
    "print(new_data2.columns.get_loc('GICS Sub Industry'))\n",
    "750/20:\n",
    "symbol_list=new_data2['symbol'].tolist()\n",
    "year_list=new_data2['For Year'].tolist()\n",
    "date_list=new_data2['date'].tolist()\n",
    "sector_list=new_data2['GICS Sector'].tolist()\n",
    "sub_industry_list=new_data2['GICS Sub Industry'].tolist()\n",
    "750/21: sns.violinplot(data=new_data2,x='For Year',y='PE')\n",
    "750/22:\n",
    "#2014 and 2015 PE\n",
    "yearsplt = ['2014','2015']\n",
    "g = sns.violinplot(data=new_data2[new_data2['For Year'].isin(yearsplt)][['PE','GICS Sector','For Year']],\n",
    "                   x='GICS Sector',y='PE',hue='For Year',split=True)\n",
    "plt.xticks(rotation=90)\n",
    "750/23:\n",
    "#PE of each sector by year\n",
    "pd.pivot_table(data=new_data2[['GICS Sector','PE','For Year']],\n",
    "               index='GICS Sector',columns='For Year',aggfunc='mean', values='PE').sort_values('2015',ascending=False)\n",
    "750/24:\n",
    "pd.pivot_table(data=new_data2[['GICS Sub Industry','PE','For Year']],\n",
    "               index='GICS Sub Industry',columns='For Year',aggfunc='mean', values='PE').sort_values('2015',ascending=False)\n",
    "750/25:\n",
    "# Add back the columns later\n",
    "new_data2.drop(['symbol','For Year','date','GICS Sector','GICS Sub Industry'], axis=1,inplace=True)\n",
    "750/26:\n",
    "#use knn to fill in the missing values of cash,quick and current ratios\n",
    "import sys\n",
    "from impyute.imputation.cs import fast_knn\n",
    "sys.setrecursionlimit(100000) #Increase the recursion limit of the OS\n",
    "\n",
    "# start the KNN training\n",
    "fastknn_data=fast_knn(new_data2.values, k=30)\n",
    "750/27:\n",
    "new_data2 = pd.DataFrame.from_records(fastknn_data, columns = new_data2.columns)\n",
    "new_data2.head()\n",
    "750/28:\n",
    "#add back the columns to the dataframe\n",
    "new_data2.insert(loc=0, column='symbol', value=symbol_list)\n",
    "new_data2.insert(loc=74, column='For Year', value=year_list)\n",
    "new_data2.insert(loc=1, column='date', value=date_list)\n",
    "new_data2.insert(loc=82, column='GICS Sector', value=sector_list)\n",
    "new_data2.insert(loc=83, column='GICS Sub Industry', value=sub_industry_list)\n",
    "750/29: new_data2['date']=pd.to_datetime(new_data2['date'])\n",
    "750/30:\n",
    "for col in ['GICS Sector','GICS Sub Industry']:\n",
    "    new_data2[col] = new_data2[col].astype('category')\n",
    "for col in ['GICS Sector','GICS Sub Industry']:\n",
    "    new_data2[col] = new_data2[col].cat.codes\n",
    "750/31: new_data2.info()\n",
    "750/32: new_data2.shape\n",
    "750/33:\n",
    "#save For Classification and Regression Problems\n",
    "new_data2.to_csv('final_merged_data.csv',date_format='%Y-%m-%d')\n",
    "750/34: new_data3=new_data2.iloc[:,2:87]\n",
    "750/35: new_data3.head()\n",
    "750/36: pca_data = new_data3.drop(['open', 'low', 'high'], axis=1)\n",
    "750/37:\n",
    "def fit_pca(df, n_components):\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(df)   \n",
    "    return pca\n",
    "750/38: pca_naive = fit_pca(pca_data, n_components=20)\n",
    "750/39:\n",
    "def plot_naive_variance(pca):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax.set_xlabel('Dimension #')\n",
    "    ax.set_ylabel('Explained Variance Ratio')\n",
    "    ax.set_title('Fraction of Explained Variance')\n",
    "    ax.plot(pca.explained_variance_ratio_)\n",
    "\n",
    "    return ax\n",
    "750/40: naive_var = plot_naive_variance(pca_naive)\n",
    "750/41:\n",
    "features = range(pca_naive.n_components_)\n",
    "plt.bar(features, pca_naive.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "750/42:\n",
    "def standardize(df):\n",
    "    stscaler = StandardScaler().fit(df)\n",
    "    scaled = stscaler.transform(df)    \n",
    "    return scaled\n",
    "750/43: scaled = standardize(pca_data)\n",
    "750/44: pca = fit_pca(scaled, n_components=20)\n",
    "750/45:\n",
    "def plot_scaled_variance(pca):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax.set_xlabel('Dimension #')\n",
    "    ax.set_ylabel('Explained Variance Ratio')\n",
    "    ax.set_title('Fraction of Explained Variance')\n",
    "    ax.plot(pca.explained_variance_ratio_)\n",
    "       \n",
    "    return ax\n",
    "750/46: ax = plot_scaled_variance(pca)\n",
    "750/47:\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "750/48:\n",
    "vars = pca.explained_variance_ratio_\n",
    "c_names = ['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10']\n",
    "\n",
    "print('Variance:  Projected dimension')\n",
    "print('------------------------------')\n",
    "for idx, row in enumerate(pca.components_):\n",
    "    output = '{0:4.1f}%:    '.format(100.0 * vars[idx])\n",
    "    output += \" + \".join(\"{0:5.2f} * {1:s}\".format(val, name) for val, name in zip(row, c_names))\n",
    "    print(output)\n",
    "750/49:\n",
    "# 83.5% are explained by PCA 20 features\n",
    "26.9+11.3+5.3+4.6+4.4+3.9+3.4+3.1+2.6+2.4+2.1+2+1.8+1.6+1.5+1.5+1.4+1.3+1.2+1.2\n",
    "750/50: reduced = pca.transform(scaled)\n",
    "750/51: scaled.shape\n",
    "750/52: reduced.shape\n",
    "750/53: new_data2.to_csv('reduced.csv')\n",
    "750/54:\n",
    "# Troy, please use \"reduced\" to run clustering. \n",
    "# Feel free to edit the PCA part, we can drop more columns when create \"pca_data\".\n",
    "750/55: data00=new_data2.copy()\n",
    "750/56: data00\n",
    "750/57:\n",
    "data2012=data00.loc[data00['For Year'] == '2012']\n",
    "data2013=data00.loc[data00['For Year'] == '2013']\n",
    "data2014=data00.loc[data00['For Year'] == '2014']\n",
    "data2015=data00.loc[data00['For Year'] == '2015']\n",
    "data2016=data00.loc[data00['For Year'] == '2016']\n",
    "750/58:\n",
    "names2012 = data2012['symbol'].values.tolist()\n",
    "from collections import OrderedDict\n",
    "names2012 = list(OrderedDict.fromkeys(names2012))\n",
    "\n",
    "names2013 = data2013['symbol'].values.tolist()\n",
    "names2013 = list(OrderedDict.fromkeys(names2013))\n",
    "\n",
    "names2014 = data2014['symbol'].values.tolist()\n",
    "names2014 = list(OrderedDict.fromkeys(names2014))\n",
    "\n",
    "names2015 = data2015['symbol'].values.tolist()\n",
    "names2015 = list(OrderedDict.fromkeys(names2015))\n",
    "\n",
    "names2016 = data2016['symbol'].values.tolist()\n",
    "names2016 = list(OrderedDict.fromkeys(names2016))\n",
    "750/59:\n",
    "data2012 = data2012.drop(['symbol', 'date','For Year','trend'], axis=1)\n",
    "data2013 = data2013.drop(['symbol', 'date','For Year','trend'], axis=1)\n",
    "data2014 = data2014.drop(['symbol', 'date','For Year','trend'], axis=1)\n",
    "data2015 = data2015.drop(['symbol', 'date','For Year','trend'], axis=1)\n",
    "data2016 = data2016.drop(['symbol', 'date','For Year','trend'], axis=1)\n",
    "750/60:\n",
    "#import needed libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "x2012 = data2012.iloc[:,0:82].values\n",
    "750/61: x2012\n",
    "750/62: data2012\n",
    "749/66:\n",
    "#k = 4\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clu)\n",
    "\n",
    "plt.scatter(clu[y_clu == 0, 0], clu[y_clu == 0, 1], s = 35, \n",
    "            c = 'orange')\n",
    "plt.scatter(clu[y_clu == 1, 0], clu[y_clu == 1, 1], s = 35, \n",
    "            c = 'blue')\n",
    "# plt.scatter(clu[y_clu == 2, 0], clu[y_clu == 2, 1], s = 35, \n",
    "#             c = 'green')\n",
    "# plt.scatter(clu[y_clu == 3, 0], clu[y_clu == 3, 1], s = 35, \n",
    "#             c = 'purple')\n",
    "\n",
    "#Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 100, \n",
    "            c = 'red', label = 'Centroide')\n",
    "\n",
    "# xk4=clu[y_clu == 1, 0]\n",
    "# yk4=clu[y_clu == 1, 1]\n",
    "# mydict = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict)\n",
    "# l4 = [names2015[i] for i in (40,  53, 180, 340)]\n",
    "# for x, y, company in zip(xk4, yk4, l4):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease dataset',fontsize=17)\n",
    "plt.legend()\n",
    "749/67:\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clu)\n",
    "\n",
    "for i in range(0,4):\n",
    "    sns.scatterplot(clu[y_clu == i, 0], clu[y_clu == i, 1])\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 50, \n",
    "            c = 'red')\n",
    "\n",
    "# xk7=xx2013[y_kmeans13 == 2, 0]\n",
    "# yk7=xx2013[y_kmeans13 == 2, 1]\n",
    "# mydict7 = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict7)\n",
    "# l7 = [pcaname2013[i] for i in (40,  53, 182, 338)]\n",
    "# for x, y, company in zip(xk7, yk7, l7):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "# xk77=xx2013[y_kmeans13 == 0, 0]\n",
    "# yk77=xx2013[y_kmeans13 == 0, 1]\n",
    "# l77 = [pcaname2013[i] for i in (85, 127, 148, 169, 253, 298, 334, 342, 350)]\n",
    "# for x, y, company in zip(xk77, yk77, l77):\n",
    "#     plt.annotate(company, (x, y), fontsize=10, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease',fontsize=17)\n",
    "749/68:\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',\n",
    "                    max_iter = 400, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(clu)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for 2015 before PCA')\n",
    "plt.xlabel('Association')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()\n",
    "749/69:\n",
    "#k = 4\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu = kmeans.fit_predict(clu)\n",
    "\n",
    "plt.scatter(clu[y_clu == 0, 0], clu[y_clu == 0, 1], s = 35, \n",
    "            c = 'orange')\n",
    "plt.scatter(clu[y_clu == 1, 0], clu[y_clu == 1, 1], s = 35, \n",
    "            c = 'blue')\n",
    "plt.scatter(clu[y_clu == 2, 0], clu[y_clu == 2, 1], s = 35, \n",
    "            c = 'green')\n",
    "plt.scatter(clu[y_clu == 3, 0], clu[y_clu == 3, 1], s = 35, \n",
    "            c = 'purple')\n",
    "\n",
    "#Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 100, \n",
    "            c = 'red', label = 'Centroide')\n",
    "\n",
    "# xk4=clu[y_clu == 1, 0]\n",
    "# yk4=clu[y_clu == 1, 1]\n",
    "# mydict = {i:np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "# print(mydict)\n",
    "# l4 = [names2015[i] for i in (40,  53, 180, 340)]\n",
    "# for x, y, company in zip(xk4, yk4, l4):\n",
    "#     plt.annotate(company, (x, y), fontsize=15, alpha=0.75)\n",
    "\n",
    "plt.title('K-means for Heart Disease dataset',fontsize=17)\n",
    "plt.legend()\n",
    "749/70: clu2 = clu.drop(['male'],axis=1)\n",
    "749/71: clu2 = DataFrame(clu).drop(['male'],axis=1)\n",
    "749/72: clu2 = pd.DataFrame(clu).drop(['male'],axis=1)\n",
    "749/73:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "749/74:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/75: df0.head()\n",
    "749/76: df0.isnull().sum()\n",
    "749/77:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/78: df0.isnull().sum()\n",
    "749/79: df0.dtypes\n",
    "749/80:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "749/81: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "749/82: clu.dtypes\n",
    "749/83: clu.isnull().sum()\n",
    "749/84: clu2 = clu.drop(['male'],axis=1)\n",
    "749/85: clu2\n",
    "749/86: clu3 = clu2.values\n",
    "749/87:\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',\n",
    "                    max_iter = 400, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(clu3)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for 2015 before PCA')\n",
    "plt.xlabel('Association')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()\n",
    "749/88:\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 500, \n",
    "                n_init = 10, random_state = 0)\n",
    "y_clu3 = kmeans.fit_predict(clu3)\n",
    "\n",
    "for i in range(0,4):\n",
    "    sns.scatterplot(clu3[y_clu3 == i, 0], clu3[y_clu3 == i, 1])\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:,1], s = 50, \n",
    "            c = 'red')\n",
    "\n",
    "plt.title('K-means for Heart Disease',fontsize=17)\n",
    "749/89:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "749/90:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "749/91: df0.head()\n",
    "749/92: df0.isnull().sum()\n",
    "749/93:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "749/94: df0.isnull().sum()\n",
    "749/95: df0.dtypes\n",
    "749/96:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "749/97: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "749/98: clu.dtypes\n",
    "749/99: clu.isnull().sum()\n",
    "749/100:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "plt.figure(figsize=(15, 10))  \n",
    "plt.title(\"Features Dendograms\")\n",
    "mergings = linkage(clu, method='complete')#ward)\n",
    "\n",
    "labels = fcluster(mergings, 70, criterion='distance')\n",
    "dendrogram(mergings,leaf_rotation=90,leaf_font_size=6)\n",
    "plt.show()\n",
    "\n",
    "print(labels)\n",
    "749/101:\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=12, affinity='euclidean', linkage='ward')  \n",
    "cluster.fit_predict(clu)\n",
    "749/102:\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.scatter(clu[:,0], clu[:,1], c=cluster.labels_, cmap='rainbow')\n",
    "754/1:\n",
    "movements=pd.read_csv(\"movements.csv\", header=None, index_col=False, skiprows=1, usecols=range(1,964))\n",
    "movements.head()\n",
    "754/2:\n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "754/3:\n",
    "movements=pd.read_csv(\"movements.csv\", header=None, index_col=False, skiprows=1, usecols=range(1,964))\n",
    "movements.head()\n",
    "754/4:\n",
    "movements=pd.read_csv(\"movements.csv\", header=None, index_col=False, skiprows=1, usecols=range(1,964))\n",
    "movements\n",
    "754/5:\n",
    "movements=pd.read_csv(\"movements.csv\")\n",
    "movements\n",
    "754/6:\n",
    "movements=pd.read_csv(\"movements.csv\", header=None, index_col=False, skiprows=1, usecols=range(1,964))\n",
    "movements.head()\n",
    "749/103:\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "\n",
    "# Import required packages for pre and post processing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus=clu.values\n",
    "# Any Pre-processing needed?\n",
    "normalized_clus = normalize(clus)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_clus, method='single')\n",
    "#ward, complete, single\n",
    "#\"Your code here\"\n",
    "# Plot the dendrogram\n",
    "dendrogram(\n",
    "    mergings,\n",
    "    labels=companies,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8\n",
    ")\n",
    "\n",
    "\n",
    "#\"Your code here\"\n",
    "plt.show()\n",
    "749/104:\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "\n",
    "# Import required packages for pre and post processing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus=clu.values\n",
    "# Any Pre-processing needed?\n",
    "normalized_clus = normalize(clus)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_clus, method='single')\n",
    "#ward, complete, single\n",
    "#\"Your code here\"\n",
    "# Plot the dendrogram\n",
    "dendrogram(\n",
    "    mergings,\n",
    "    \n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8\n",
    ")\n",
    "# labels=companies,\n",
    "\n",
    "#\"Your code here\"\n",
    "plt.show()\n",
    "749/105: clu\n",
    "754/7:\n",
    "dataset = pd.read_csv('Iris.csv')\n",
    "#\n",
    "\n",
    "\n",
    "x = dataset.iloc[:, [1, 2, 3, 4]].values\n",
    "species_iris=dataset.iloc[:,[5]]#.values[:,0]\n",
    "754/8: species_iris\n",
    "749/106:\n",
    "x = clu.values\n",
    "class0=df0[['TenYearCHD']]#.values[:,0]\n",
    "749/107: class0\n",
    "749/108:\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "model=TSNE(learning_rate=100)\n",
    "Tsne_transformed=model.fit_transform(x)\n",
    "755/1:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "755/2:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "755/3: df0.head()\n",
    "755/4: df0.isnull().sum()\n",
    "755/5:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "755/6: df0.isnull().sum()\n",
    "755/7: df0.dtypes\n",
    "755/8:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "755/9: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "755/10: clu.dtypes\n",
    "755/11: clu.isnull().sum()\n",
    "755/12:\n",
    "x = clu.values\n",
    "class0=df0[['']]#.values[:,0]\n",
    "755/13:\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "model=TSNE(learning_rate=100)\n",
    "Tsne_transformed=model.fit_transform(x)\n",
    "755/14:\n",
    "x = clu.values\n",
    "class0=df0[['TenYearCHD']]#.values[:,0]\n",
    "755/15: clu\n",
    "755/16:\n",
    "index0=list[0:4238]\n",
    "index0\n",
    "755/17:\n",
    "index0=range(4238)\n",
    "index0\n",
    "755/18:\n",
    "index0=np.arange(0, 4238).tolist()\n",
    "index0\n",
    "755/19: index0=np.arange(0, 4238).tolist()\n",
    "755/20:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs2012, ys2012, alpha=0.5)\n",
    "# Annotate the points\n",
    "for x0, y0, index0 in zip(xs, ys, names):\n",
    "    plt.annotate(index0, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=17)\n",
    "plt.show()\n",
    "755/21:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys2, alpha=0.5)\n",
    "# Annotate the points\n",
    "for x0, y0, index0 in zip(xs, ys, names):\n",
    "    plt.annotate(index0, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=17)\n",
    "plt.show()\n",
    "755/22:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "# Annotate the points\n",
    "for x0, y0, index0 in zip(xs, ys, names):\n",
    "    plt.annotate(index0, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=17)\n",
    "plt.show()\n",
    "755/23:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=17)\n",
    "plt.show()\n",
    "755/24:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5,c=class0)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=10)\n",
    "plt.show()\n",
    "754/9: species=species_iris['Species'].astype('category').cat.codes\n",
    "754/10: species\n",
    "755/25:\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "# model=TSNE(learning_rate=100)\n",
    "# Tsne_transformed=model.fit_transform(x)\n",
    "755/26: class0\n",
    "755/27:\n",
    "class0=class0['TenYearCHD'].astype('category').cat.codes\n",
    "class0\n",
    "755/28: class0=class0['TenYearCHD'].astype('category').cat.codes\n",
    "755/29:\n",
    "x = clu.values\n",
    "class0=df0[['TenYearCHD']]#.values[:,0]\n",
    "755/30: class0=class0['TenYearCHD'].astype('category').cat.codes\n",
    "755/31:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5,c=class0)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=10)\n",
    "plt.show()\n",
    "755/32:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5,c=class0)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=9, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=4)\n",
    "plt.show()\n",
    "755/33:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5,c=class0)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=4, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=14)\n",
    "plt.show()\n",
    "755/34:\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "clus = clu.values\n",
    "normalized_clus = normalize(clus)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "tsne = TSNE(learning_rate=100)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = tsne.fit_transform(normalized_clus)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5,c=class0)\n",
    "# Annotate the points\n",
    "for x0, y0, patient in zip(xs, ys, index0):\n",
    "    plt.annotate(patient, (x0, y0), fontsize=6, alpha=0.75)\n",
    "#plt.figure(figsize=(18,20))\n",
    "plt.title('t-SNE for heart disease',fontsize=14)\n",
    "plt.show()\n",
    "755/35:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print \"The continuous distance \" + numeric_distance + \" is not supported.\"\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print \"The binary distance \" + categorical_distance + \" is not supported.\"\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print \"Not enough observations.\"\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print \"The number of observations in the attributes variable is not matching the target variable length.\"\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print \"The range of the number of neighbors is incorrect.\"\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print \"The aggregation method is incorrect.\"\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print \"The only method allowed for categorical target variable is the mode.\"\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "755/36:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "755/37:\n",
    "import matplotlib\n",
    "matplotlib.use('GTKAgg')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 6\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# prepare data\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "h = .02\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# calculate min, max and limits\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "np.arange(y_min, y_max, h))\n",
    "\n",
    "# predict class using data and kNN classifier\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\" % (n_neighbors))\n",
    "plt.show()\n",
    "755/38:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "755/39: df0['education']\n",
    "755/40: df0['education'].loc[[33]]\n",
    "755/41: df0['education'].loc[[32]]\n",
    "755/42: df0['education'].loc[[0]]\n",
    "755/43: df0['education'].loc[[1]]\n",
    "755/44: df0['education'].loc[[8]]\n",
    "755/45: df0\n",
    "755/46: df0['education'].loc[[4]]\n",
    "755/47: df0['education'].loc[[30]]\n",
    "755/48: df0['education'].loc[[31]]\n",
    "755/49: df0['education'].loc[[29]]\n",
    "755/50: df0['education'].loc[[28]]\n",
    "755/51: df0['education'].loc[[27]]\n",
    "755/52: df0['education'].loc[[26]]\n",
    "755/53: df0['education'].loc[[25]]\n",
    "755/54: df0['education'].loc[[30]]\n",
    "755/55: df0['education'].loc[[31]]\n",
    "755/56: df0['education'].loc[[32]]\n",
    "755/57: df0['education'].loc[[33]]\n",
    "755/58: df0['education'].loc[[36]]\n",
    "755/59: # df0['education'].loc[[36]]\n",
    "755/60:\n",
    "# df0['education'].loc[[36]]\n",
    "df0.isnull().sum()\n",
    "755/61: clu2 = df0.drop(['glucose','TenYearCHD'],axis=1)\n",
    "755/62: clu2\n",
    "755/63:\n",
    "# df0['education'].loc[[36]]\n",
    "df0.isnull().sum()\n",
    "755/64:\n",
    "df0['education'] = df0['education'].astype('category').cat.codes\n",
    "df0.dtypes\n",
    "755/65:\n",
    "glu = df0.dropna()\n",
    "glu\n",
    "755/66:\n",
    "glu = df0.dropna()\n",
    "glu.shape\n",
    "755/67: glu_test = df0[df0[['glucose']].isna()]\n",
    "755/68:\n",
    "glu_test = df0[df0[['glucose']].isna()]\n",
    "glu_test\n",
    "755/69:\n",
    "glu_test = df0[df0['glucose'].isnull()]\n",
    "glu_test\n",
    "755/70:\n",
    "glu_test = df0[df0['glucose'].isnull()]\n",
    "glu_test.shape\n",
    "755/71:\n",
    "glu_x = glu.drop(['glucose'],axis=1)\n",
    "glu_y = glu[['glucose']]\n",
    "755/72: glu_x\n",
    "755/73:\n",
    "glu_x = glu.drop(['glucose','TenYearCHD'],axis=1)\n",
    "glu_y = glu[['glucose']]\n",
    "755/74: glu_x\n",
    "755/75: glu_y\n",
    "755/76:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "glu_xtrain, glu_xtest, glu_ytrain, glu_ytest = train_test_split(glu_x, glu_y, test_size=0.075, random_state=SEED)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(glu_xtrain, glu_ytrain)\n",
    "reg.score(glu_xtrain, glu_ytrain)\n",
    "755/77:\n",
    "glu_pred0 = reg.predict(glu_xtest)\n",
    "glu_pred_proba0 = reg.predict_proba(glu_xtest)[:,1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "reg_roc_auc = roc_auc_score(glu_ytest, glu_pred_proba0)\n",
    "755/78:\n",
    "glu_pred0 = reg.predict(glu_xtest)\n",
    "# glu_pred_proba0 = reg.predict_proba(glu_xtest)[:,1]\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# reg_roc_auc = roc_auc_score(glu_ytest, glu_pred_proba0)\n",
    "755/79:\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr_accuracy_score0 = metrics.accuracy_score(glu_ytest, glu_pred0)\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy_score0)\n",
    "755/80: glu_ytest\n",
    "755/81: glu_pred0\n",
    "755/82: glu_ytest['glucose'] = glu_ytest['glucose'].astype('float').cat.codes\n",
    "755/83: glu_ytest['glucose'] = glu_ytest['glucose'].astype('float')\n",
    "755/84: glu_ytest\n",
    "755/85:\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr_accuracy_score0 = metrics.accuracy_score(glu_ytest, glu_pred0)\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy_score0)\n",
    "755/86:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "rmse_glu = MSE(glu_ytest, glu_pred0)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu)\n",
    "755/87:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(100, 1100, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,11)],\n",
    "              'max_depth':[1,2,3],\n",
    "              'gamma': [i/4 for i in range(0,21)]\n",
    "              }\n",
    "755/88:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(100, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb = XGBRegressor(random_state = random_state)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb = GridSearchCV(gxbt_obj, param_grid, cv = 5, \n",
    "                   refit = True,\n",
    "                   n_jobs=-1, verbose = 5)\n",
    "\n",
    "%time model4_gxbt_clf.fit(glu_xtrain, glu_ytrain)\n",
    "755/89:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(100, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb = GridSearchCV(gxbt_obj, param_grid, cv = 5, \n",
    "                   refit = True,\n",
    "                   n_jobs=-1, verbose = 5)\n",
    "\n",
    "model4_gxbt_clf.fit(glu_xtrain, glu_ytrain)\n",
    "755/90:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(100, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(gxbt_obj, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "755/91:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(100, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "755/92:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "755/93:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,3)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "755/94:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,3)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "764/1:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/2:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/3: df0.head()\n",
    "764/4: df0.isnull().sum()\n",
    "764/5:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/6: df0.isnull().sum()\n",
    "764/7: df0.dtypes\n",
    "764/8:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/9: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/10: clu.dtypes\n",
    "764/11: clu.isnull().sum()\n",
    "764/12:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/13:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/14:\n",
    "# df0['education'].loc[[36]]\n",
    "df0.isnull().sum()\n",
    "764/15:\n",
    "df0['education'] = df0['education'].astype('category').cat.codes\n",
    "df0.dtypes\n",
    "764/16:\n",
    "glu_test = df0[df0['glucose'].isnull()]\n",
    "glu_test.shape\n",
    "764/17:\n",
    "glu = df0.dropna()\n",
    "glu.shape\n",
    "764/18:\n",
    "glu_x = glu.drop(['glucose','TenYearCHD'],axis=1)\n",
    "glu_y = glu[['glucose']]\n",
    "764/19:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "glu_xtrain, glu_xtest, glu_ytrain, glu_ytest = train_test_split(glu_x, glu_y, test_size=0.075, random_state=SEED)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(glu_xtrain, glu_ytrain)\n",
    "reg.score(glu_xtrain, glu_ytrain)\n",
    "764/20:\n",
    "glu_pred0 = reg.predict(glu_xtest)\n",
    "# glu_pred_proba0 = reg.predict_proba(glu_xtest)[:,1]\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# reg_roc_auc = roc_auc_score(glu_ytest, glu_pred_proba0)\n",
    "764/21: glu_ytest['glucose'] = glu_ytest['glucose'].astype('float')\n",
    "764/22:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "rmse_glu = MSE(glu_ytest, glu_pred0)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu)\n",
    "764/23:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,3)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "764/24: glu_pred1 = xgb0.best_estimator_.predict(glu_xtest)\n",
    "764/25:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "rmse_glu0 = MSE(glu_ytest, glu_pred0)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu0)\n",
    "764/26:\n",
    "glu_pred1 = xgb0.best_estimator_.predict(glu_xtest)\n",
    "rmse_glu1 = MSE(glu_ytest, glu_pred1)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu1)\n",
    "764/27:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,3)],\n",
    "              'max_depth':[2,4,5],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "764/28:\n",
    "glu_pred1 = xgb0.best_estimator_.predict(glu_xtest)\n",
    "rmse_glu1 = MSE(glu_ytest, glu_pred1)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu1)\n",
    "764/29:\n",
    "# import library\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# direct xgboost library and possibly use \"cv\" from library\n",
    "import xgboost as xgb\n",
    "# sklearn wrapper for XGBoost. Allows Grid Search parellel processing like GBM\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# create a dictionary of parameters using range(start, stop but not including, step) \n",
    "param_grid = {'n_estimators': list(range(300, 500, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,3)],\n",
    "              'max_depth':[2,3],\n",
    "              'gamma': [0.1,0.5,1,5]\n",
    "              }\n",
    "xgb0 = XGBRegressor(random_state = 1)\n",
    "\n",
    "# create randomizedsearchCV object with various combinations of parameters\n",
    "xgb0 = GridSearchCV(xgb0, param_grid, cv = 5, \n",
    "                    refit = True,\n",
    "                    n_jobs=-1, verbose = 5)\n",
    "\n",
    "xgb0.fit(glu_xtrain, glu_ytrain)\n",
    "xgb0.best_estimator_\n",
    "764/30:\n",
    "glu_pred1 = xgb0.best_estimator_.predict(glu_xtest)\n",
    "rmse_glu1 = MSE(glu_ytest, glu_pred1)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu1)\n",
    "764/31:\n",
    "from sklearn.ensemble import GradientBoostingRegresssor\n",
    "\n",
    "# create a dictionary of parameters \n",
    "param_grid = {'n_estimators':list(range(400, 700, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3]\n",
    "              }\n",
    "# create AdaBoostClassifier model\n",
    "gbr0 = GradientBoostingRegresssor(random_state=1)\n",
    "\n",
    "# create gridsearch object with various combinations of parameters\n",
    "gbr0 = RandomizedSearchCV(gbr0, param_grid, cv = 5,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1, verbose = 5)\n",
    "\n",
    "gbr0.fit(X_train, y_train)\n",
    "gbr0.fit(glu_xtrain, glu_ytrain)\n",
    "gbr0.best_estimator_\n",
    "764/32:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create a dictionary of parameters \n",
    "param_grid = {'n_estimators':list(range(400, 700, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3]\n",
    "              }\n",
    "# create AdaBoostClassifier model\n",
    "gbr0 = GradientBoostingRegresssor(random_state=1)\n",
    "\n",
    "# create gridsearch object with various combinations of parameters\n",
    "gbr0 = RandomizedSearchCV(gbr0, param_grid, cv = 5,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1, verbose = 5)\n",
    "\n",
    "gbr0.fit(X_train, y_train)\n",
    "gbr0.fit(glu_xtrain, glu_ytrain)\n",
    "gbr0.best_estimator_\n",
    "764/33:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create a dictionary of parameters \n",
    "param_grid = {'n_estimators':list(range(400, 700, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3]\n",
    "              }\n",
    "# create AdaBoostClassifier model\n",
    "gbr0 = GradientBoostingRegressor(random_state=1)\n",
    "       \n",
    "# create gridsearch object with various combinations of parameters\n",
    "gbr0 = RandomizedSearchCV(gbr0, param_grid, cv = 5,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1, verbose = 5)\n",
    "\n",
    "gbr0.fit(X_train, y_train)\n",
    "gbr0.fit(glu_xtrain, glu_ytrain)\n",
    "gbr0.best_estimator_\n",
    "764/34:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create a dictionary of parameters \n",
    "param_grid = {'n_estimators':list(range(400, 700, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3]\n",
    "              }\n",
    "# create AdaBoostClassifier model\n",
    "gbr0 = GradientBoostingRegressor(random_state=1)\n",
    "       \n",
    "# create gridsearch object with various combinations of parameters\n",
    "gbr0 = RandomizedSearchCV(gbr0, param_grid, cv = 5,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1, verbose = 5)\n",
    "\n",
    "gbr0.fit(glu_xtrain, glu_ytrain)\n",
    "gbr0.best_estimator_\n",
    "764/35:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create a dictionary of parameters \n",
    "param_grid = {'n_estimators':list(range(400, 700, 100)),\n",
    "              'learning_rate':[i/10.0 for i in range(1,5)],\n",
    "              'max_depth':[2,3]\n",
    "              }\n",
    "# create AdaBoostClassifier model\n",
    "gbr0 = GradientBoostingRegressor(random_state=1)\n",
    "       \n",
    "# create gridsearch object with various combinations of parameters\n",
    "gbr0 = GridSearchCV(gbr0, param_grid, cv = 5,\n",
    "                          refit = True,\n",
    "                          n_jobs=-1, verbose = 5)\n",
    "\n",
    "gbr0.fit(glu_xtrain, glu_ytrain)\n",
    "gbr0.best_estimator_\n",
    "764/36:\n",
    "glu_pred2 = gbr0.best_estimator_.predict(glu_xtest)\n",
    "rmse_glu2 = MSE(glu_ytest, glu_pred1)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu2)\n",
    "764/37:\n",
    "glu_pred2 = gbr0.best_estimator_.predict(glu_xtest)\n",
    "rmse_glu2 = MSE(glu_ytest, glu_pred2)**(1/2)\n",
    "print(\"Linear Regression RMSE:\", rmse_glu2)\n",
    "764/38: glu_ytrain\n",
    "764/39: glu_xtrain\n",
    "764/40: glu_x\n",
    "764/41:\n",
    "glu_x['glucose']=knn_impute(target=glu_x['glucose'], attributes=glu_x,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/42:\n",
    "missing = reg.predict(glu_test)\n",
    "missing\n",
    "764/43: glu_test\n",
    "764/44: glu_test = glu_test.drop(['glucose','TenYearCHD'],axis=1)\n",
    "764/45:\n",
    "missing = reg.predict(glu_test)\n",
    "missing\n",
    "764/46: glu_test['glucose']=missing\n",
    "764/47: glu_test\n",
    "764/48:\n",
    "glu_test = df0[df0['glucose'].isnull()]\n",
    "glu_test.shape\n",
    "764/49: glu_test\n",
    "764/50:\n",
    "glu_test = df0[df0['glucose'].isnull()]\n",
    "glu_test.shape\n",
    "764/51: glu_test_y=glu_test[['TenYearCHD']]\n",
    "764/52: glu_test = glu_test.drop(['glucose','TenYearCHD'],axis=1)\n",
    "764/53:\n",
    "missing = reg.predict(glu_test)\n",
    "missing\n",
    "764/54: glu_test['glucose']=missing\n",
    "764/55: glu_test\n",
    "764/56:\n",
    "glu_test['TenYearCHD'] = glu_test_y\n",
    "glu_test\n",
    "764/57: glu.shape\n",
    "764/58: df = pd.concat([glu, glu_test], ignore_index=False)\n",
    "764/59: df\n",
    "764/60: df.isnull().sum()\n",
    "764/61: df.iloc[[12:28]]\n",
    "764/62: df.iloc[[12:28],:]\n",
    "764/63: df.iloc[12:28,:]\n",
    "764/64: df = pd.concat([glu, glu_test], ignore_index=False).sort_values(df.index)\n",
    "764/65: df.index\n",
    "764/66: df.shape\n",
    "764/67: df.iloc[2840:3860,:]\n",
    "764/68: df.iloc[2845:3855,:]\n",
    "764/69: df.iloc[3845:3855,:]\n",
    "764/70:\n",
    "df = df.set_index('column_name', append=True).sort_index(level=1).reset_index(level=1)\n",
    "df.iloc[3845:3855,:]\n",
    "764/71:\n",
    "df = df.set_index('column_name2', append=True).sort_index(level=1).reset_index(level=1)\n",
    "df.iloc[3845:3855,:]\n",
    "764/72:\n",
    "df['colFromIndex'] = df.index\n",
    "df = df.sort(['colFromIndex'])\n",
    "df.iloc[3845:3855,:]\n",
    "764/73:\n",
    "df = df.rename_axis('MyIdx').sort_values(by = ['MyIdx'], ascending = [True])\n",
    "df.iloc[3845:3855,:]\n",
    "764/74:\n",
    "df = df.rename_axis('MyIdx').sort_values(by = ['MyIdx'], ascending = [True])\n",
    "df.iloc[12:28,:]\n",
    "764/75: df = pd.concat([glu, glu_test], ignore_index=False)\n",
    "764/76: df.isnull().sum()\n",
    "764/77: df.iloc[3845:3855,:]\n",
    "764/78:\n",
    "df = df.rename_axis('MyIdx').sort_values(by = ['MyIdx'], ascending = [True])\n",
    "df.iloc[12:28,:]\n",
    "764/79: # df.iloc[3845:3855,:]\n",
    "764/80: df.isnull().sum()\n",
    "764/81: df.shape\n",
    "764/82: df.iloc[3845:3855,:]\n",
    "764/83: # df.iloc[3845:3855,:]\n",
    "764/84: df.dtypes\n",
    "764/85: pd.DataFrame(df['glucose'].describe())\n",
    "764/86: pd.DataFrame(df['heartRate'].describe())\n",
    "764/87: pd.DataFrame(df['BMI'].describe())\n",
    "764/88:\n",
    "print(df['glucose'].skew())\n",
    "print(df['glucose'].kurt())\n",
    "764/89:\n",
    "print(df['glucose'].skew())\n",
    "print(df['glucose'].kurt())\n",
    "sns.distplot(df['glucose'])\n",
    "764/90:\n",
    "print(df['TenYearCHD'].skew())\n",
    "print(df['TenYearCHD'].kurt())\n",
    "sns.distplot(df['TenYearCHD'])\n",
    "764/91:\n",
    "from scipy.stats import norm\n",
    "\n",
    "#setting transformed dependent variable with a new name\n",
    "df['glucose'] = np.log(df['glucose'])\n",
    "\n",
    "sns.distplot(df['glucose'], fit=norm)\n",
    "764/92:\n",
    "df['TenYearCHD'] = np.log(df['TenYearCHD'])\n",
    "sns.distplot(df['TenYearCHD'], fit=norm)\n",
    "764/93:\n",
    "corrmat = df.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)\n",
    "766/1:\n",
    "#Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "766/2:\n",
    "#Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model, metrics\n",
    "# from sklearn import cross_validation\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "766/3: !pip3 install --upgrade sklearn\n",
    "766/4:\n",
    "df_raw = pd.read_csv(\"train.csv\", engine = 'c')\n",
    "df_raw.isnull().sum()\n",
    "764/94:\n",
    "missing = reg.predict(glu_test)\n",
    "missing.head()\n",
    "764/95:\n",
    "#Plots scatterplots and histograms for the most highly correlated variables to \n",
    "cols = ['glucose', 'diabetes', 'age', 'sysBP', 'BMI', 'heartRate', 'BPMeds']\n",
    "sns.pairplot(df_raw[cols], size = 2.5)\n",
    "plt.show()\n",
    "764/96:\n",
    "#Plots scatterplots and histograms for the most highly correlated variables to \n",
    "cols = ['glucose', 'diabetes', 'age', 'sysBP', 'BMI', 'heartRate', 'BPMeds']\n",
    "sns.pairplot(df[cols], size = 2.5)\n",
    "plt.show()\n",
    "764/97:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'Neighborhood'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"Glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "764/98:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'education'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"Glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "764/99:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'education'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "764/100:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'education'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=0, ymax=400);\n",
    "764/101:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'education'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=25, ymax=200);\n",
    "764/102:\n",
    "#box plot of Neighborhood, sorted by glucose\n",
    "var = 'education'\n",
    "data = pd.concat([np.exp(df['glucose']), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ranks = df.groupby(var)['glucose'].mean().fillna(0).sort_values()[::-1].index\n",
    "fig = sns.boxplot(x=var, y=\"glucose\", data=data, order=ranks)\n",
    "plt.xticks(rotation=45)\n",
    "fig.axis(ymin=25, ymax=180);\n",
    "764/103:\n",
    "a = df['education'].unique()\n",
    "print(sorted(a))\n",
    "764/104:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/105:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/106: df0.head()\n",
    "764/107: df0.isnull().sum()\n",
    "764/108:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/109: df0.isnull().sum()\n",
    "764/110: df0.dtypes\n",
    "764/111:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/112: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/113: clu.dtypes\n",
    "764/114: clu.isnull().sum()\n",
    "764/115:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/116:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/117:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/118: df0\n",
    "764/119: df0[df0['education']==1.5]\n",
    "764/120: df0[df0['education']==2.5]\n",
    "764/121:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"mode\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/122:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/123: f0[df0['education']==1.5]\n",
    "764/124: df0[df0['education']==1.5]\n",
    "764/125:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/126:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/127: df0[df0['education']==1.5]\n",
    "764/128: df0[df0['education']==2.5]\n",
    "764/129: df0['education'].loc[[943]]\n",
    "764/130:\n",
    "df0['education'].loc[[306]]\n",
    "df0['education'].loc[[1604]]\n",
    "764/131:\n",
    "print(df0['education'].loc[[306]], df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[1604]],  df0['education'].loc[[1604]])\n",
    "764/132:\n",
    "print(df0['education'].loc[[306]], df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[2885]],  df0['education'].loc[[4012]])\n",
    "764/133:\n",
    "print(df0['education'].loc[[306]], 'n\\',df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[2885]],  df0['education'].loc[[4012]])\n",
    "764/134:\n",
    "print(df0['education'].loc[[306]], 'n/', df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[2885]],  df0['education'].loc[[4012]])\n",
    "764/135:\n",
    "print(df0['education'].loc[[306]], '/n', df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[2885]],  df0['education'].loc[[4012]])\n",
    "764/136:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], \n",
    "      df0['education'].loc[[2885]],  df0['education'].loc[[4012]])\n",
    "764/137:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/138:\n",
    "def education_adjust(df):\n",
    "    if df0['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker = 1.0:\n",
    "        return 1.0\n",
    "    elif df0['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker = 1.0:\n",
    "        return 2.0\n",
    "    elif df0['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker = 0):\n",
    "        return 2.0\n",
    "df0['education'] = df.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/139:\n",
    "def education_adjust(df):\n",
    "    if df0['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        return 1.0\n",
    "    elif df0['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        return 2.0\n",
    "    elif df0['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        return 2.0\n",
    "df0['education'] = df.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/140:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        return 2.0\n",
    "df0['education'] = df.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/141:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/142: df0.head()\n",
    "764/143:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        df['education'] == 2.0\n",
    "df0['education'] = df.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/144: df0.head()\n",
    "764/145:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/146:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/147:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/148: df0.head()\n",
    "764/149: df0.isnull().sum()\n",
    "764/150:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/151: df0.isnull().sum()\n",
    "764/152: df0.dtypes\n",
    "764/153:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/154: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/155: clu.dtypes\n",
    "764/156: clu.isnull().sum()\n",
    "764/157:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/158:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/159:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/160: df0[df0['education']==1.5]\n",
    "764/161: df0['education'].loc[[943]]\n",
    "764/162: df0[df0['education']==2.5]\n",
    "764/163:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/164:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        df['education'] == 2.0\n",
    "df0['education'] = df.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/165: df0.head()\n",
    "764/166:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/167:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/168: df0.head()\n",
    "764/169: df0.isnull().sum()\n",
    "764/170:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/171: df0.isnull().sum()\n",
    "764/172: df0.dtypes\n",
    "764/173:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/174: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/175: clu.dtypes\n",
    "764/176: clu.isnull().sum()\n",
    "764/177:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/178:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/179:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/180: df0[df0['education']==1.5]\n",
    "764/181: df0['education'].loc[[943]]\n",
    "764/182: df0[df0['education']==2.5]\n",
    "764/183:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/184:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        df['education'] == 2.0\n",
    "df0['education'] = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/185:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/186:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/187: df0.head()\n",
    "764/188: df0.isnull().sum()\n",
    "764/189:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/190: df0.isnull().sum()\n",
    "764/191: df0.dtypes\n",
    "764/192:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/193: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/194: clu.dtypes\n",
    "764/195: clu.isnull().sum()\n",
    "764/196:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/197:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/198:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/199: df0[df0['education']==1.5]\n",
    "764/200: df0['education'].loc[[943]]\n",
    "764/201: df0[df0['education']==2.5]\n",
    "764/202:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/203:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and currentSmoker == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or currentSmoker == 0):\n",
    "        df['education'] == 2.0\n",
    "df0 = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/204:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/205: df0.head()\n",
    "764/206: df0.isnull().sum()\n",
    "764/207:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/208: df0.isnull().sum()\n",
    "764/209: df0.dtypes\n",
    "764/210:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/211: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/212: clu.dtypes\n",
    "764/213: clu.isnull().sum()\n",
    "764/214:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/215:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/216:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/217: df0[df0['education']==1.5]\n",
    "764/218: df0['education'].loc[[943]]\n",
    "764/219: df0[df0['education']==2.5]\n",
    "764/220:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/221:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 2.0\n",
    "df0 = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/222:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        return == 3.0\n",
    "df0['education'] = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/223:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        return 3.0\n",
    "df0['education'] = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/224:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        return 3.0\n",
    "df0['education2'] = df0.apply (lambda df0: education_adjust(df), axis=1)\n",
    "764/225:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df0 = df0.apply (lambda df0: education_adjust(df0), axis=1)\n",
    "764/226: df0.head()\n",
    "764/227:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/228:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/229:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/230: df0.head()\n",
    "764/231: df0.isnull().sum()\n",
    "764/232:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/233: df0.isnull().sum()\n",
    "764/234: df0.dtypes\n",
    "764/235:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/236: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/237: clu.dtypes\n",
    "764/238: clu.isnull().sum()\n",
    "764/239:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/240:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/241:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/242: df0[df0['education']==1.5]\n",
    "764/243: df0['education'].loc[[943]]\n",
    "764/244: df0[df0['education']==2.5]\n",
    "764/245:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/246:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df0 = df0.apply (lambda df0: education_adjust(df0), axis=1)\n",
    "764/247: df0.head()\n",
    "764/248:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/249: df0.head()\n",
    "764/250: df0.isnull().sum()\n",
    "764/251:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/252: df0.isnull().sum()\n",
    "764/253: df0.dtypes\n",
    "764/254:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/255: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/256: clu.dtypes\n",
    "764/257: clu.isnull().sum()\n",
    "764/258:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/259:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/260:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/261: df0[df0['education']==1.5]\n",
    "764/262: df0['education'].loc[[943]]\n",
    "764/263: df0[df0['education']==2.5]\n",
    "764/264:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/265: df11 = df0.copy()\n",
    "764/266:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/267: df11.head()\n",
    "764/268: df11 = df0.copy()\n",
    "764/269: df11.head()\n",
    "764/270:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda df: education_adjust(df11), axis=1)\n",
    "764/271: df11 = df0.copy()\n",
    "764/272:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda x: education_adjust(df11), axis=1)\n",
    "764/273: df11 = df0.copy()\n",
    "764/274:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/275: df11.head()\n",
    "764/276: df11 = df0.copy()\n",
    "764/277:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda x: education_adjust(x), axis=1)\n",
    "764/278: df11.head()\n",
    "764/279: df11 = df0.copy()\n",
    "764/280:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (lambda df: education_adjust(df))\n",
    "764/281: df11['education']\n",
    "764/282: df11 = df0.copy()\n",
    "764/283:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11 = df11.apply (education_adjust)\n",
    "764/284:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11['education'] = df11.apply (education_adjust)\n",
    "764/285:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/286: df11.head()\n",
    "764/287: df11 = df0.copy()\n",
    "764/288:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/289: df11.head()\n",
    "764/290:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/291: df11[df11['education']==nan]\n",
    "764/292: df11[df11['education']==Nan]\n",
    "764/293: df11[df11['education']=='nan']\n",
    "764/294: df11[df11['education']=='Nan']\n",
    "764/295: df11[df11['education']=='null']\n",
    "764/296:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/297: df11[df11['education']=='NaN']\n",
    "764/298: df11[df11['education']==float('nan')]\n",
    "764/299: df11[df11['education']==nan]\n",
    "764/300: df11[df11['education']==float('nan')]\n",
    "764/301: df11.shape\n",
    "764/302: df11['education'].loc[[943]]\n",
    "764/303: df11 = df0.copy()\n",
    "764/304:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        return 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/305: df11.head()\n",
    "764/306:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/307: df11 = df0.copy()\n",
    "764/308:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/309: df11.head()\n",
    "764/310:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/311: df11['education'].loc[[943]]\n",
    "764/312:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/313: df11.head()\n",
    "764/314:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/315: df0.head()\n",
    "764/316:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/317: df0['education'].loc[[943]]\n",
    "764/318:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/319:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/320:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/321: df0.head()\n",
    "764/322: df0.isnull().sum()\n",
    "764/323:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/324: df0.isnull().sum()\n",
    "764/325: df0.dtypes\n",
    "764/326:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/327: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/328: clu.dtypes\n",
    "764/329: clu.isnull().sum()\n",
    "764/330:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/331:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/332:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/333: df0[df0['education']==1.5]\n",
    "764/334: df0['education'].loc[[943]]\n",
    "764/335: df0[df0['education']==2.5]\n",
    "764/336:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/337: # df11 = df0.copy()\n",
    "764/338:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/339: df0.head()\n",
    "764/340:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/341:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/342:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/343: df0.head()\n",
    "764/344: df0.isnull().sum()\n",
    "764/345:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/346: df0.isnull().sum()\n",
    "764/347: df0.dtypes\n",
    "764/348:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/349: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/350: clu.dtypes\n",
    "764/351: clu.isnull().sum()\n",
    "764/352:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/353:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/354:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/355: df0[df0['education']==1.5]\n",
    "764/356: df0['education'].loc[[943]]\n",
    "764/357: df0[df0['education']==2.5]\n",
    "764/358:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/359: # df11 = df0.copy()\n",
    "764/360:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/361: df0.head()\n",
    "764/362:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/363:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/364: df0.head()\n",
    "764/365: df0.isnull().sum()\n",
    "764/366:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/367: df0.isnull().sum()\n",
    "764/368: df0.dtypes\n",
    "764/369:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/370: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/371: clu.dtypes\n",
    "764/372: clu.isnull().sum()\n",
    "764/373:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/374:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/375:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/376: df0[df0['education']==1.5]\n",
    "764/377: df0['education'].loc[[943]]\n",
    "764/378: df0[df0['education']==2.5]\n",
    "764/379:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/380: # df11 = df0.copy()\n",
    "764/381:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/382: df0.head()\n",
    "764/383:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/384: df0['education'].loc[[943]]\n",
    "764/385:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/386: df0.head()\n",
    "764/387:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/388: df0['education'].loc[[943]]\n",
    "764/389:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/390:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/391: df0.head()\n",
    "764/392: df0.isnull().sum()\n",
    "764/393:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/394: df0.isnull().sum()\n",
    "764/395: df0.dtypes\n",
    "764/396:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/397: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/398: clu.dtypes\n",
    "764/399: clu.isnull().sum()\n",
    "764/400:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/401:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/402:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/403: df0[df0['education']==1.5]\n",
    "764/404: df0['education'].loc[[943]]\n",
    "764/405: df0[df0['education']==2.5]\n",
    "764/406:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/407: df11 = df0.copy()\n",
    "764/408:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/409: df11.head()\n",
    "764/410:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/411: df11['education'].loc[[943]]\n",
    "764/412:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/413: df11 = df0.copy()\n",
    "764/414:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9 and df['currentSmoker'] == 1.0:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df0['education'] = df0.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/415: df11.head()\n",
    "764/416:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/417:\n",
    "b = df0['education'].unique()\n",
    "print(sorted(b))\n",
    "764/418: df11['education'].loc[[943]]\n",
    "764/419:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/420:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/421: df0.head()\n",
    "764/422: df0.isnull().sum()\n",
    "764/423:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/424: df0.isnull().sum()\n",
    "764/425: df0.dtypes\n",
    "764/426:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/427: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/428: clu.dtypes\n",
    "764/429: clu.isnull().sum()\n",
    "764/430:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "764/431:\n",
    "df0['education']=knn_impute(target=df0['education'], attributes=clu,\n",
    "                            aggregation_method=\"median\", k_neighbors=9, numeric_distance='euclidean',\n",
    "                            categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "764/432:\n",
    "a = df0['education'].unique()\n",
    "print(sorted(a))\n",
    "764/433: df0[df0['education']==1.5]\n",
    "764/434: df0['education'].loc[[943]]\n",
    "764/435: df0[df0['education']==2.5]\n",
    "764/436:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/437: df11 = df0.copy()\n",
    "764/438:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9.0 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/439: df11.head()\n",
    "764/440:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/441: df11 = df0.copy()\n",
    "764/442:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker']) == 1:\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker']) == 1:\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9.0 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/443: df11.head()\n",
    "764/444:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/445: df11 = df0.copy()\n",
    "764/446:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1):\n",
    "        df['education'] == 1.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1):\n",
    "        df['education'] == 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9.0 or df['currentSmoker'] == 0):\n",
    "        df['education'] == 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/447: df11.head()\n",
    "764/448:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/449: df11['education'].loc[[943]]\n",
    "764/450: df11 = df0.copy()\n",
    "764/451:\n",
    "def education_adjust(df):\n",
    "    if df['education']==1.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1):\n",
    "        return 1.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']>=9.0 and df['currentSmoker'] == 1):\n",
    "        return 2.0\n",
    "    elif df['education']==2.5 and (df['cigsPerDay']<9.0 or df['currentSmoker'] == 0):\n",
    "        return 3.0\n",
    "    else:\n",
    "        return df['education']\n",
    "df11['education'] = df11.apply (lambda df: education_adjust(df), axis=1)\n",
    "764/452: df11.head()\n",
    "764/453:\n",
    "b = df11['education'].unique()\n",
    "print(sorted(b))\n",
    "764/454: df11['education'].loc[[943]]\n",
    "764/455:\n",
    "print(df0['education'].loc[[306]], '\\n', df0['education'].loc[[1604]], '\\n',\n",
    "      df0['education'].loc[[2885]], '\\n', df0['education'].loc[[4012]])\n",
    "764/456:\n",
    "print(df11['education'].loc[[306]], '\\n', df11['education'].loc[[1604]], '\\n',\n",
    "      df11['education'].loc[[2885]], '\\n', df11['education'].loc[[4012]])\n",
    "764/457:\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "764/458:\n",
    "df0=pd.read_csv('Heart_disease.csv')\n",
    "df0.shape\n",
    "764/459: df0.head()\n",
    "764/460: df0.isnull().sum()\n",
    "764/461:\n",
    "df0['cigsPerDay']=df0['cigsPerDay'].fillna(df0['cigsPerDay'].mean())\n",
    "df0['BPMeds']=df0['BPMeds'].fillna(0)\n",
    "df0['totChol']=df0['totChol'].fillna(df0['totChol'].mean())\n",
    "df0['BMI']=df0['BMI'].fillna(df0['BMI'].mean())\n",
    "df0['heartRate']=df0['heartRate'].fillna(df0['heartRate'].mean())\n",
    "764/462: df0.isnull().sum()\n",
    "764/463: df0.dtypes\n",
    "764/464:\n",
    "df0['male'] = df0['male'].astype('category').cat.codes\n",
    "df0['currentSmoker'] = df0['currentSmoker'].astype('category').cat.codes\n",
    "df0['BPMeds'] = df0['BPMeds'].astype('int').astype('category').cat.codes\n",
    "df0['prevalentStroke'] = df0['prevalentStroke'].astype('category').cat.codes\n",
    "df0['prevalentHyp'] = df0['prevalentHyp'].astype('category').cat.codes\n",
    "df0['diabetes'] = df0['diabetes'].astype('category').cat.codes\n",
    "764/465: clu = df0.drop(['education','glucose','TenYearCHD'],axis=1)\n",
    "764/466: clu.dtypes\n",
    "764/467: clu.isnull().sum()\n",
    "764/468:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
