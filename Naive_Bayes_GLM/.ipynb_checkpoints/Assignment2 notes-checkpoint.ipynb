{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ridge\n",
    "ridge_model = ridge(alpha = 1, solved=\"cholesky\")\n",
    "ridge_model = RidgeClassifier(alpha = 1)\n",
    "\n",
    "ridge_model.fit(X_train, y_train)\n",
    "test_predictions_log.predict(X_test)\n",
    "auc = roc_auc_Score(y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitcontrol <- \n",
    "reg.model train default~.,\n",
    "data-train,method=\"glmnet\",\n",
    "tuneGrid=objGrid, trControl=fitControl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds predict(object=reg.model, d.test, type=\"pro\")\n",
    "d.default.test$predicted_prob_default<-preds$Yes\n",
    "reg_perf <- roc(response = d.test$default,predictor=d.test$predicted_prob_default)\n",
    "\n",
    "print(pROC::auc(reg_perf))\n",
    "print(pROC::ci.auc(reg_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_ytest10 = np.empty(read_ytest1.shape[0], dtype=object)\n",
    "read_ytest10[:] = read_ytest1.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(x, f, nsamples=1000):\n",
    "    stats = [f(x[np.random.randint(x.shape[0], size=x.shape[0])]) for _ in range(nsamples)]\n",
    "    return np.percentile(stats, (2.5, 97.5))\n",
    "def bootstrap_auc(clf, X_train, y_train, X_test, y_test, nsamples=1000):\n",
    "    auc_values = []\n",
    "    for b in range(nsamples):\n",
    "        idx = np.random.randint(X_train.shape[0], size=X_train.shape[0])\n",
    "        clf.fit(X_train[idx], y_train[idx])\n",
    "        pred = clf.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())\n",
    "        auc_values.append(roc_auc)\n",
    "    return np.percentile(auc_values, (2.5, 97.5))\n",
    "bootstrap_auc(naive, read_Xtrain1, read_ytrain1, read_Xtest1, read_ytest1, nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "mean_confidence_interval(y_pred1, confidence=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Original ROC area: {:0.3f}\".format(roc_auc_score(y_true, y_pred)))\n",
    "\n",
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "    if len(np.unique(y_true[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "    print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #R\n",
    "# read_train1 = read_train[['PatientEncounterAge','PatientGender','PatientRace','outcome']]\n",
    "# read_train1.to_csv(\"read_train1.csv\")\n",
    "# read_test1 = read_test[['PatientEncounterAge','PatientGender','PatientRace','outcome']]\n",
    "# read_test1.to_csv(\"read_test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read = read[['PatientEncounterAge','PatientGender','PatientRace','outcome']]\n",
    "# read.to_csv(\"read.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
